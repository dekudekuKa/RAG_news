import re
from pymongo import MongoClient
from bson.objectid import ObjectId
import stanza
import datetime

# üîπ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MongoDB
client = MongoClient("mongodb://localhost:27017")
db = client["news_database"]
raw_collection = db["raw_articles"]
clean_collection = db["clean_articles"]

# üîπ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó NLP –º–æ–¥–µ–ª—ñ
stanza.download('uk')  # —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –∑–∞–ø—É—Å–∫—É
nlp = stanza.Pipeline('uk', processors='tokenize,mwt,pos,lemma', use_gpu=False)

# üîπ –í–ª–∞—Å–Ω—ñ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
ukrainian_stopwords = {
    '—ñ', '–≤', '—É', '—Ç–∞', '–Ω–∞', '–∑', '—â–æ', '—Ü–µ', '–¥–æ', '–ø–æ', '—è', '–º–∏', '–≤–∏', '–≤–æ–Ω–∏',
    '–ø—Ä–æ', '—è–∫', '–∑–∞', '–≤—ñ–¥', '–¥–ª—è', '–Ω–µ', '–∞', '–∞–ª–µ', '—â–µ', '–±—É—Ç–∏', '—Ç–∞–∫–∏–π', '–∂',
    '—Ç–∞–∫', '–π–æ–≥–æ', '—ó—ó', '—ó—Ö', '—á–∏', '—è–∫–∏–π', '—è–∫–∞', '—è–∫–µ', '—è–∫—ñ', '—Ç—É—Ç', '—Ç–∞–º',
    '—á–µ—Ä–µ–∑', '–ø—ñ—Å–ª—è', '–ø–µ—Ä–µ–¥', '–º—ñ–∂', '—Ç–µ–∂', '–∂', '–æ—Ç–æ', '–Ω—ñ', '—Ç–∞–∫–æ–∂', '—Ö—Ç–æ', '–∫–æ–ª–∏',
    '—á–æ–º—É', '–¥–µ', '—ñ–∑', '–Ω–∞–¥', '–ø—ñ–¥', '–æ–±', '—Ç–æ—â–æ', '—â–æ–±', '—Å–∞–º–µ'
}

# üîπ –§—É–Ω–∫—Ü—ñ—è –æ—á–∏—â–µ–Ω–Ω—è –π –ª–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó
def clean_text(text):
    text = re.sub(r"[^–ê-–Ø–∞-—è–á—ó–Ü—ñ–Ñ—î“ê“ë\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    doc = nlp(text)
    lemmas = []

    for sentence in doc.sentences:
        for word in sentence.words:
            lemma = word.lemma.lower()
            if lemma not in ukrainian_stopwords and len(lemma) > 2:
                lemmas.append(lemma)

    return " ".join(lemmas), lemmas  # —ñ —Å—Ç—Ä–æ–∫–∞, —ñ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤

# üîπ –û—Å–Ω–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞
def process_articles():
    raw_docs = raw_collection.find()
    count = 0
    skipped = 0

    for raw in raw_docs:
        # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: text > content > summary
        text = raw.get("text") or raw.get("content") or raw.get("summary", "")
        if not text.strip():
            print(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ—Ä–æ–∂–Ω—ñ–π —Ç–µ–∫—Å—Ç: {raw.get('_id')}")
            skipped += 1
            continue

        if clean_collection.find_one({"raw_id": raw["_id"]}):
            skipped += 1
            continue

        cleaned_str, tokens = clean_text(text)
        if not cleaned_str.strip():
            print(f"‚ö† –ü–æ—Ä–æ–∂–Ω—ñ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏: {raw.get('_id')}")
            skipped += 1
            continue

        clean_doc = {
            "raw_id": raw["_id"],
            "clean_text": cleaned_str,
            "tokens": tokens,
            "lemmatized_at": datetime.datetime.utcnow()
        }

        clean_collection.insert_one(clean_doc)
        print(f"‚úì –û–±—Ä–æ–±–ª–µ–Ω–æ: {raw.get('title', '')}")
        count += 1

    print(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –£—Å–ø—ñ—à–Ω–æ: {count}, –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}")



if __name__ == "__main__":
    process_articles()
